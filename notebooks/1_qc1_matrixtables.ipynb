{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2706a40",
   "metadata": {},
   "source": [
    "# Create a MatrixTable and QC the hell out of it\n",
    "## Import stuff and set your parameters\n",
    "First, we import necessary libraries and configurations from config.toml. Then we initialise Spark and Hail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34df66ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "from datetime import datetime\n",
    "from distutils.version import LooseVersion\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "\n",
    "import dxdata\n",
    "import dxpy\n",
    "import hail as hl\n",
    "import pandas as pd\n",
    "import pyspark\n",
    "import tomli\n",
    "from matrixtables import *\n",
    "from utils import get_stats\n",
    "\n",
    "Path(\"../tmp\").resolve().mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a3782bb",
   "metadata": {
    "lines_to_next_cell": 2,
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "with open(\"../config.toml\", \"rb\") as f:\n",
    "    conf = tomli.load(f)\n",
    "\n",
    "IMPORT = conf[\"IMPORT\"]\n",
    "NAME = conf[\"NAME\"]\n",
    "VCF_VERSION = IMPORT[\"VCF_VERSION\"]\n",
    "REFERENCE_GENOME = conf[\"REFERENCE_GENOME\"]\n",
    "DATABASE = IMPORT[\"DATABASE\"]\n",
    "\n",
    "LOG_FILE = (\n",
    "    Path(IMPORT[\"LOG_DIR\"], f\"{NAME}_{datetime.now().strftime('%H%M')}.log\")\n",
    "    .resolve()\n",
    "    .__str__()\n",
    ")\n",
    "\n",
    "MAP_FILE = Path(IMPORT[\"MAPPING_FILE\"]).resolve().__str__()\n",
    "INT_FILE = Path(IMPORT[\"INTERVAL_FILE\"]).resolve().__str__()\n",
    "GENE_FILE = Path(IMPORT[\"GENE_FILE\"]).resolve().__str__()\n",
    "\n",
    "with open(GENE_FILE, \"r\") as file:\n",
    "    GENES = file.read().splitlines()\n",
    "\n",
    "    if NAME == \"NONE\":\n",
    "        NAME = GENES[0]\n",
    "\n",
    "\n",
    "VCF_DIR = Path(IMPORT[\"VCF_DIR\"]).resolve().__str__()\n",
    "\n",
    "DOWNSAMPLE_P = IMPORT.get(\"DOWNSAMPLE_P\", None)\n",
    "\n",
    "SNV_ONLY = conf[\"ANNOTATE\"][\"SNV_ONLY\"]\n",
    "USE_VEP = conf[\"ANNOTATE\"][\"USE_VEP\"]\n",
    "MISSENSE_ONLY = conf[\"ANNOTATE\"][\"MISSENSE_ONLY\"]\n",
    "\n",
    "VEP_JSON = Path(conf[\"ANNOTATE\"][\"VEP_JSON\"]).resolve().__str__()\n",
    "\n",
    "TMP_DIR = conf[\"EXPORT\"][\"TMP_DIR\"]\n",
    "\n",
    "BGEN_FILE = Path(TMP_DIR, f\"{NAME}\").resolve().__str__()\n",
    "ANNOTATIONS_FILE = Path(TMP_DIR, f\"{NAME}.annotations\").resolve().__str__()\n",
    "SETLIST_FILE = Path(TMP_DIR, f\"{NAME}.setlist\").resolve().__str__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9a20784",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pip-installed Hail requires additional configuration options in Spark referring\n",
      "  to the path to the Hail Python module directory HAIL_DIR,\n",
      "  e.g. /path/to/python/site-packages/hail:\n",
      "    spark.jars=HAIL_DIR/hail-all-spark.jar\n",
      "    spark.driver.extraClassPath=HAIL_DIR/hail-all-spark.jar\n",
      "    spark.executor.extraClassPath=./hail-all-spark.jarRunning on Apache Spark version 2.4.4\n",
      "SparkUI available at http://ip-10-60-25-134.eu-west-2.compute.internal:8081\n",
      "Welcome to\n",
      "     __  __     <>__\n",
      "    / /_/ /__  __/ /\n",
      "   / __  / _ `/ / /\n",
      "  /_/ /_/\\_,_/_/_/   version 0.2.61-3c86d3ba497a\n",
      "LOGGING: writing to /opt/notebooks/gogoGPCR/hail_logs/NONE_1947.log\n"
     ]
    }
   ],
   "source": [
    "# Spark and Hail\n",
    "\n",
    "sc = pyspark.SparkContext()\n",
    "spark = pyspark.sql.SparkSession(sc)\n",
    "\n",
    "try:\n",
    "    mt_database = dxpy.find_one_data_object(name=DATABASE)[\"id\"]\n",
    "except Exception as e:\n",
    "    spark.sql(f\"CREATE DATABASE {DATABASE} LOCATION  'dnax://'\")\n",
    "    mt_database = dxpy.find_one_data_object(name=DATABASE)[\"id\"]\n",
    "\n",
    "# this breaks export_bgen for now\n",
    "# hl.init(sc=sc, default_reference=REFERENCE_GENOME, log=LOG_FILE, tmp_dir=f'dnax://{mt_database}/tmp/')\n",
    "\n",
    "hl.init(sc=sc, default_reference=REFERENCE_GENOME, log=LOG_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72318df4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'260 variants and 200643 samples after import'\n"
     ]
    }
   ],
   "source": [
    "# Import\n",
    "mapping = pd.read_csv(MAP_FILE, sep=\"\\t\").set_index(\"HGNC\", drop=False)\n",
    "\n",
    "mt = import_mt(GENES, mapping, vcf_dir=VCF_DIR, vcf_version=VCF_VERSION).key_rows_by(\n",
    "    \"locus\", \"alleles\"\n",
    ")  # .checkpoint(checkpoint_file)\n",
    "\n",
    "v, s = mt.count()\n",
    "pprint(f\"{v} variants and {s} samples after import\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee343797",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-05 19:49:40 Hail: INFO: Coerced sorted dataset\n",
      "2021-12-05 19:50:30 Hail: INFO: wrote matrix table with 260 rows and 200643 columns in 1 partition to /tmp/MC4R.RAW.cp.mt\n",
      "    Total size: 246.04 MiB\n",
      "    * Rows/entries: 244.73 MiB\n",
      "    * Columns: 1.31 MiB\n",
      "    * Globals: 11.00 B\n",
      "    * Smallest partition: 260 rows (244.73 MiB)\n",
      "    * Largest partition:  260 rows (244.73 MiB)\n"
     ]
    }
   ],
   "source": [
    "# Checkpoint\n",
    "stage = \"RAW\"\n",
    "checkpoint_file = f\"/tmp/{NAME}.{stage}.cp.mt\"\n",
    "\n",
    "mt = mt.checkpoint(checkpoint_file, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c5f9fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downsample\n",
    "if DOWNSAMPLE_P is not None:\n",
    "    mt = downsample_mt(mt, DOWNSAMPLE_P)\n",
    "\n",
    "    pprint(f\"{mt.count_cols()} samples after downsampling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ff6c4ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-05 19:52:00 Hail: INFO: Reading table without type imputation\n",
      "  Loading field 'f0' as type str (user-supplied)\n",
      "  Loading field 'f1' as type int32 (user-supplied)\n",
      "  Loading field 'f2' as type int32 (user-supplied)\n",
      "2021-12-05 19:52:03 Hail: INFO: Coerced sorted dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'204 variants after interval filtering'\n"
     ]
    }
   ],
   "source": [
    "# Interval QC\n",
    "mt = interval_qc_mt(mt, \"file:\" + INT_FILE)\n",
    "\n",
    "pprint(f\"{mt.count_rows()} variants after interval filtering\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d52ca1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-05 19:52:08 Hail: INFO: Coerced sorted dataset\n",
      "2021-12-05 19:52:10 Hail: INFO: Coerced sorted dataset\n",
      "2021-12-05 19:52:13 Hail: INFO: Coerced sorted dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'223 variants with not more than 6 alleles after splitting'\n"
     ]
    }
   ],
   "source": [
    "# Split multi\n",
    "mt = mt.filter_rows(mt.alleles.length() <= 6)\n",
    "mt = smart_split_multi_mt(mt)\n",
    "\n",
    "pprint(f\"{mt.count_rows()} variants with not more than 6 alleles after splitting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e528630d",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-05 19:52:17 Hail: INFO: Coerced sorted dataset\n",
      "2021-12-05 19:52:19 Hail: INFO: Coerced sorted dataset\n",
      "2021-12-05 19:52:20 Hail: INFO: Coerced sorted dataset\n",
      "2021-12-05 19:52:22 Hail: INFO: Coerced sorted dataset\n",
      "2021-12-05 19:52:23 Hail: INFO: Coerced sorted dataset\n",
      "2021-12-05 19:52:25 Hail: INFO: Coerced sorted dataset\n",
      "2021-12-05 19:52:26 Hail: INFO: Coerced sorted dataset\n",
      "2021-12-05 19:52:27 Hail: INFO: Coerced sorted dataset\n"
     ]
    }
   ],
   "source": [
    "if USE_VEP:\n",
    "    mt = hl.vep(mt, \"file:\" + VEP_JSON)\n",
    "\n",
    "    is_MANE = mt.aggregate_rows(\n",
    "        hl.agg.all(hl.is_defined(mt.vep.transcript_consequences.mane_select))\n",
    "    )\n",
    "    assert is_MANE, \"Selected transcript may not be MANE Select. Check manually.\"\n",
    "\n",
    "    mt = mt.annotate_rows(\n",
    "        protCons=mt.vep.transcript_consequences.amino_acids[0].split(\"/\")[0]\n",
    "        + hl.str(mt.vep.transcript_consequences.protein_end[0])\n",
    "        + mt.vep.transcript_consequences.amino_acids[0].split(\"/\")[-1]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a016a782",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-05 19:54:09 Hail: INFO: Coerced sorted dataset\n",
      "2021-12-05 19:54:11 Hail: INFO: Coerced sorted dataset\n",
      "2021-12-05 19:54:19 Hail: INFO: Coerced sorted dataset\n",
      "2021-12-05 19:54:20 Hail: INFO: Coerced sorted dataset\n",
      "2021-12-05 19:56:03 Hail: INFO: wrote matrix table with 223 rows and 200643 columns in 3 partitions to dnax://database-G6XB998J860kZy4z59fBqPBV/MC4R.QC1.mt\n",
      "    Total size: 113.68 MiB\n",
      "    * Rows/entries: 112.55 MiB\n",
      "    * Columns: 1.13 MiB\n",
      "    * Globals: 11.00 B\n",
      "    * Smallest partition: 0 rows (20.00 B)\n",
      "    * Largest partition:  210 rows (102.36 MiB)\n"
     ]
    }
   ],
   "source": [
    "STAGE = \"QC1\"\n",
    "WRITE_PATH = \"dnax://\" + mt_database + f\"/{NAME}.{STAGE}.mt\"\n",
    "\n",
    "mt.write(WRITE_PATH, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76296dda",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
