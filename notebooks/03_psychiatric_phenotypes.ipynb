{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "599d3b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import subprocess\n",
    "\n",
    "import dxdata\n",
    "import dxpy\n",
    "import pandas as pd\n",
    "import pyspark\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.conf import SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69f7547e",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf()\n",
    "conf.set(\"autoBroadcastJoinThreshold\", -1)\n",
    "\n",
    "sc = pyspark.SparkContext()\n",
    "spark = pyspark.sql.SparkSession(sc)\n",
    "\n",
    "dispensed_database_name = dxpy.find_one_data_object(\n",
    "    classname=\"database\", name=\"app*\", folder=\"/\", name_mode=\"glob\", describe=True\n",
    ")[\"describe\"][\"name\"]\n",
    "dispensed_dataset_id = dxpy.find_one_data_object(\n",
    "    typename=\"Dataset\", name=\"app*.dataset\", folder=\"/\", name_mode=\"glob\"\n",
    ")[\"id\"]\n",
    "\n",
    "spark.sql(\"USE \" + dispensed_database_name)\n",
    "\n",
    "dataset = dxdata.load_dataset(id=dispensed_dataset_id)\n",
    "participant = dataset[\"participant\"]\n",
    "\n",
    "\n",
    "def get_columns_to_keep(df, threshold=200) -> list:\n",
    "    \"\"\"\n",
    "    This function drops all columns which contain more null values than threshold\n",
    "    :param df: A PySpark DataFrame\n",
    "    \"\"\"\n",
    "    null_counts = (\n",
    "        df.select([F.count(F.when(~F.col(c).isNull(), c)).alias(c) for c in df.columns])\n",
    "        .collect()[0]\n",
    "        .asDict()\n",
    "    )\n",
    "    to_keep = [k for k, v in null_counts.items() if v > threshold]\n",
    "    # df = df.select(*to_keep)\n",
    "\n",
    "    return to_keep\n",
    "\n",
    "\n",
    "def new_names(s: str) -> str:\n",
    "    \"\"\"\n",
    "    Fixes a column header for PHESANT use\n",
    "    \"\"\"\n",
    "    s = s.replace(\"p\", \"x\").replace(\"i\", \"\")\n",
    "\n",
    "    if bool(re.search(\"_\\d$\", s)):\n",
    "        s += \"_0\"\n",
    "    else:\n",
    "        s += \"_0_0\"\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a55b86ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_occurences = list(\n",
    "    participant.find_fields(\n",
    "        lambda f: bool(re.match(\"^Date [F]\\d{2} first reported\", f.title))\n",
    "    )\n",
    ")\n",
    "\n",
    "age_sex = \"|\".join([\"31\", \"21022\"])\n",
    "age_sex_fields = list(\n",
    "    participant.find_fields(lambda f: bool(re.match(f\"^p({age_sex})$\", f.name)))\n",
    ")\n",
    "\n",
    "psych = \"|\".join(\n",
    "    [\n",
    "        \"2090\",\n",
    "        \"2100\",\n",
    "        \"20126\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "psych_fields = list(\n",
    "    participant.find_fields(lambda f: bool(re.match(f\"^p({psych})\\D\", f.name)))\n",
    ")\n",
    "\n",
    "field_names = (\n",
    "    [\"eid\"]\n",
    "    + [f.name for f in age_sex_fields]\n",
    "    + [f.name for f in first_occurences]\n",
    "    + [f.name for f in psych_fields]\n",
    ")\n",
    "df = participant.retrieve_fields(names=field_names, engine=dxdata.connect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c3d8724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of columns: 88\n",
      "Number of columns with at least 500 cases: 39\n"
     ]
    }
   ],
   "source": [
    "min_cases = 500\n",
    "\n",
    "print(f\"Number of columns: {len(df.columns)}\")\n",
    "to_keep = get_columns_to_keep(df, min_cases)\n",
    "\n",
    "to_keep.insert(1, to_keep[11])\n",
    "to_keep.insert(2, to_keep[11])\n",
    "to_keep.pop(12)\n",
    "to_keep.pop(12)\n",
    "\n",
    "df = df.select(*to_keep)\n",
    "\n",
    "print(f\"Number of columns with at least {min_cases} cases: {len(df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "69423979",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'list' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-434531234ecc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnew_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'xeid'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnew_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mnew_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34mf'\"{i}\"'\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnew_names\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-50-434531234ecc>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnew_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'xeid'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnew_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mnew_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34mf'\"{i}\"'\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnew_names\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'list' object is not callable"
     ]
    }
   ],
   "source": [
    "colnames = ['xeid'] + [new_names(s) for s in df.columns[1:]]\n",
    "colnames = [f'\"{i}\"' for i in colnames]\n",
    "print(colnames[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2f4730c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.toDF(*colnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34f733ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(\n",
    "    \"x130836_0_0\", \"x130838_0_0\", \"x130842_0_0\"\n",
    ")  # these do not converge during Firth correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2911c685",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.csv(\"/tmp/phenos.csv\", sep=\",\", header=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0862bdc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['hadoop', 'fs', '-get', '/tmp/phenos.csv', '../tmp/phenos.csv'], returncode=0)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subprocess.run(\n",
    "    [\"hadoop\", \"fs\", \"-rm\", \"/tmp/phenos.csv/_SUCCESS\"], check=True, shell=False\n",
    ")\n",
    "subprocess.run(\n",
    "    [\"hadoop\", \"fs\", \"-get\", \"/tmp/phenos.csv\", \"../tmp/phenos.csv\"],\n",
    "    check=True,\n",
    "    shell=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "06b83362",
   "metadata": {},
   "outputs": [],
   "source": [
    "!sed -e '3,${/^xeid/d' -e '}' ../tmp/phenos.csv/part* > ../tmp/psychiatric.raw.csv\n",
    "!sed -i 's|\"\"|NA|g' ../tmp/psychiatric.raw.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "02a626c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['dx', 'upload', '../tmp/psychiatric.raw.csv', '--path', 'Data/phenotypes/'], returncode=0)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Upload to project\n",
    "\n",
    "subprocess.run(\n",
    "    [\"dx\", \"upload\", \"../tmp/psychiatric.raw.csv\", \"--path\", \"Data/phenotypes/\"],\n",
    "    check=True,\n",
    "    shell=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7a902fbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mquote\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mescape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mnullValue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mescapeQuotes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mquoteAll\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdateFormat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtimestampFormat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mignoreLeadingWhiteSpace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mignoreTrailingWhiteSpace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcharToEscapeQuoteEscaping\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0memptyValue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Saves the content of the :class:`DataFrame` in CSV format at the specified path.\n",
       "\n",
       ":param path: the path in any Hadoop supported file system\n",
       ":param mode: specifies the behavior of the save operation when data already exists.\n",
       "\n",
       "    * ``append``: Append contents of this :class:`DataFrame` to existing data.\n",
       "    * ``overwrite``: Overwrite existing data.\n",
       "    * ``ignore``: Silently ignore this operation if data already exists.\n",
       "    * ``error`` or ``errorifexists`` (default case): Throw an exception if data already \\\n",
       "        exists.\n",
       "\n",
       ":param compression: compression codec to use when saving to file. This can be one of the\n",
       "                    known case-insensitive shorten names (none, bzip2, gzip, lz4,\n",
       "                    snappy and deflate).\n",
       ":param sep: sets a single character as a separator for each field and value. If None is\n",
       "            set, it uses the default value, ``,``.\n",
       ":param quote: sets a single character used for escaping quoted values where the\n",
       "              separator can be part of the value. If None is set, it uses the default\n",
       "              value, ``\"``. If an empty string is set, it uses ``u0000`` (null character).\n",
       ":param escape: sets a single character used for escaping quotes inside an already\n",
       "               quoted value. If None is set, it uses the default value, ``\\``\n",
       ":param escapeQuotes: a flag indicating whether values containing quotes should always\n",
       "                     be enclosed in quotes. If None is set, it uses the default value\n",
       "                     ``true``, escaping all values containing a quote character.\n",
       ":param quoteAll: a flag indicating whether all values should always be enclosed in\n",
       "                  quotes. If None is set, it uses the default value ``false``,\n",
       "                  only escaping values containing a quote character.\n",
       ":param header: writes the names of columns as the first line. If None is set, it uses\n",
       "               the default value, ``false``.\n",
       ":param nullValue: sets the string representation of a null value. If None is set, it uses\n",
       "                  the default value, empty string.\n",
       ":param dateFormat: sets the string that indicates a date format. Custom date formats\n",
       "                   follow the formats at ``java.text.SimpleDateFormat``. This\n",
       "                   applies to date type. If None is set, it uses the\n",
       "                   default value, ``yyyy-MM-dd``.\n",
       ":param timestampFormat: sets the string that indicates a timestamp format. Custom date\n",
       "                        formats follow the formats at ``java.text.SimpleDateFormat``.\n",
       "                        This applies to timestamp type. If None is set, it uses the\n",
       "                        default value, ``yyyy-MM-dd'T'HH:mm:ss.SSSXXX``.\n",
       ":param ignoreLeadingWhiteSpace: a flag indicating whether or not leading whitespaces from\n",
       "                                values being written should be skipped. If None is set, it\n",
       "                                uses the default value, ``true``.\n",
       ":param ignoreTrailingWhiteSpace: a flag indicating whether or not trailing whitespaces from\n",
       "                                 values being written should be skipped. If None is set, it\n",
       "                                 uses the default value, ``true``.\n",
       ":param charToEscapeQuoteEscaping: sets a single character used for escaping the escape for\n",
       "                                  the quote character. If None is set, the default value is\n",
       "                                  escape character when escape and quote characters are\n",
       "                                  different, ``\\0`` otherwise..\n",
       ":param encoding: sets the encoding (charset) of saved csv files. If None is set,\n",
       "                 the default UTF-8 charset will be used.\n",
       ":param emptyValue: sets the string representation of an empty value. If None is set, it uses\n",
       "                   the default value, ``\"\"``.\n",
       "\n",
       ">>> df.write.csv(os.path.join(tempfile.mkdtemp(), 'data'))\n",
       "\n",
       ".. versionadded:: 2.0\n",
       "\u001b[0;31mFile:\u001b[0m      /cluster/spark/python/pyspark/sql/readwriter.py\n",
       "\u001b[0;31mType:\u001b[0m      method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.write.csv?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7cfe277",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
