{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6bda355",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import subprocess\n",
    "\n",
    "import dxdata\n",
    "import dxpy\n",
    "import pandas as pd\n",
    "import pyspark\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c330e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = pyspark.SparkContext()\n",
    "spark = pyspark.sql.SparkSession(sc)\n",
    "\n",
    "dispensed_database_name = dxpy.find_one_data_object(\n",
    "    classname=\"database\", name=\"app*\", folder=\"/\", name_mode=\"glob\", describe=True\n",
    ")[\"describe\"][\"name\"]\n",
    "dispensed_dataset_id = dxpy.find_one_data_object(\n",
    "    typename=\"Dataset\", name=\"app*.dataset\", folder=\"/\", name_mode=\"glob\"\n",
    ")[\"id\"]\n",
    "\n",
    "spark.sql(\"USE \" + dispensed_database_name)\n",
    "\n",
    "dataset = dxdata.load_dataset(id=dispensed_dataset_id)\n",
    "participant = dataset[\"participant\"]\n",
    "\n",
    "\n",
    "def get_columns_to_keep(df, threshold=200) -> list:\n",
    "    \"\"\"\n",
    "    This function drops all columns which contain more null values than threshold\n",
    "    :param df: A PySpark DataFrame\n",
    "    \"\"\"\n",
    "    null_counts = (\n",
    "        df.select([F.count(F.when(~F.col(c).isNull(), c)).alias(c) for c in df.columns])\n",
    "        .collect()[0]\n",
    "        .asDict()\n",
    "    )\n",
    "    to_keep = [k for k, v in null_counts.items() if v > threshold]\n",
    "    # df = df.select(*to_keep)\n",
    "\n",
    "    return to_keep\n",
    "\n",
    "\n",
    "def new_names(s: str) -> str:\n",
    "    \"\"\"\n",
    "    Fixes a column header for PHESANT use\n",
    "    \"\"\"\n",
    "    s = s.replace(\"p\", \"x\").replace(\"i\", \"\")\n",
    "\n",
    "    if bool(re.search(\"_\\d$\", s)):\n",
    "        s += \"_0\"\n",
    "    else:\n",
    "        s += \"_0_0\"\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52c0297",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_occurences = list(\n",
    "    participant.find_fields(\n",
    "        lambda f: bool(re.match(\"^Date [F]\\d{2} first reported\", f.title))\n",
    "    )\n",
    ")\n",
    "\n",
    "age_sex = \"|\".join([\"31\", \"21022\"])\n",
    "age_sex_fields = list(\n",
    "    participant.find_fields(lambda f: bool(re.match(f\"^p({age_sex})$\", f.name)))\n",
    ")\n",
    "\n",
    "psych = \"|\".join(\n",
    "    [\n",
    "        \"2090\",\n",
    "        \"2100\",\n",
    "        \"20126\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "psych_fields = list(\n",
    "    participant.find_fields(lambda f: bool(re.match(f\"^p({psych})\\D\", f.name)))\n",
    ")\n",
    "\n",
    "field_names = (\n",
    "    [\"eid\"]\n",
    "    + [f.name for f in age_sex_fields]\n",
    "    + [f.name for f in first_occurences]\n",
    "    + [f.name for f in psych_fields]\n",
    ")\n",
    "df = participant.retrieve_fields(names=field_names, engine=dxdata.connect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a75850",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_cases = 500\n",
    "\n",
    "print(f\"Number of columns: {len(df.columns)}\")\n",
    "to_keep = get_columns_to_keep(df, min_cases)\n",
    "\n",
    "to_keep.insert(1, to_keep[11])\n",
    "to_keep.insert(2, to_keep[11])\n",
    "to_keep.pop(12)\n",
    "to_keep.pop(12)\n",
    "\n",
    "\n",
    "df = df.select(*to_keep)\n",
    "print(f\"Number of columns with at least {min_cases} cases: {len(df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6b6e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_names = [\"xeid\"] + [new_names(s) for s in df.columns[1:]]\n",
    "print(new_names[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4512744c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.toDF(*new_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84ee65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.csv(\"/tmp/phenos.csv\", sep=\",\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081afbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "subprocess.run(\n",
    "    [\"hadoop\", \"fs\", \"-rm\", \"/tmp/phenos.csv/_SUCCESS\"], check=True, shell=False\n",
    ")\n",
    "subprocess.run(\n",
    "    [\"hadoop\", \"fs\", \"-get\", \"/tmp/phenos.csv\", \"phenos.csv\"], check=True, shell=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343b33e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!sed -e '3,${/^xeid/d' -e '}' phenos.csv/part* > phenos.filtered.csv"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
