{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db8446c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import subprocess\n",
    "\n",
    "import dxdata\n",
    "import dxpy\n",
    "import pandas as pd\n",
    "import pyspark\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "Path(\"../tmp\").resolve().mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "trait = \"metabolic\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90a5f0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf()\n",
    "conf.set(\"autoBroadcastJoinThreshold\", -1)\n",
    "\n",
    "sc = pyspark.SparkContext()\n",
    "spark = pyspark.sql.SparkSession(sc)\n",
    "\n",
    "dispensed_database_name = dxpy.find_one_data_object(\n",
    "    classname=\"database\", name=\"app*\", folder=\"/\", name_mode=\"glob\", describe=True\n",
    ")[\"describe\"][\"name\"]\n",
    "dispensed_dataset_id = dxpy.find_one_data_object(\n",
    "    typename=\"Dataset\", name=\"app*.dataset\", folder=\"/\", name_mode=\"glob\"\n",
    ")[\"id\"]\n",
    "\n",
    "spark.sql(\"USE \" + dispensed_database_name)\n",
    "\n",
    "dataset = dxdata.load_dataset(id=dispensed_dataset_id)\n",
    "participant = dataset[\"participant\"]\n",
    "\n",
    "\n",
    "def get_columns_to_keep(df, threshold=200) -> list:\n",
    "    \"\"\"\n",
    "    This function drops all columns which contain more null values than threshold\n",
    "    :param df: A PySpark DataFrame\n",
    "    \"\"\"\n",
    "    null_counts = (\n",
    "        df.select([F.count(F.when(~F.col(c).isNull(), c)).alias(c) for c in df.columns])\n",
    "        .collect()[0]\n",
    "        .asDict()\n",
    "    )\n",
    "    to_keep = [k for k, v in null_counts.items() if v > threshold]\n",
    "    # df = df.select(*to_keep)\n",
    "\n",
    "    return to_keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7483508",
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_names(s: str) -> str:\n",
    "    \"\"\"\n",
    "    Fixes a column header for PHESANT use\n",
    "    \"\"\"\n",
    "    s = s.replace(\"p\", \"x\").replace(\"i\", \"\")\n",
    "\n",
    "    if bool(re.search(\"_\\d$\", s)):\n",
    "        s += \"_0\"\n",
    "    else:\n",
    "        s += \"_0_0\"\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "945fd2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "age_sex = \"|\".join([\"31\", \"21022\"])\n",
    "age_sex_fields = list(\n",
    "    participant.find_fields(lambda f: bool(re.match(f\"^p({age_sex})$\", f.name)))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cf34528d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[dxdata.dataset.dataset] WARNING: Dataframe schema name(s): ['participant_0001$eid', 'participant_0001$p31', 'participant_0042$p21022', 'participant_0001$p48_i0', 'participant_0001$p48_i1', 'participant_0001$p48_i2', 'participant_0001$p48_i3', 'participant_0001$p50_i0', 'participant_0001$p50_i1', 'participant_0001$p50_i2', 'participant_0001$p50_i3', 'participant_0002$p102_i0_a0', 'participant_0002$p102_i0_a1', 'participant_0002$p102_i1_a0', 'participant_0002$p102_i1_a1', 'participant_0002$p102_i2_a0', 'participant_0002$p102_i2_a1', 'participant_0002$p102_i3_a0', 'participant_0002$p102_i3_a1', 'participant_0008$p4079_i0_a0', 'participant_0008$p4079_i0_a1', 'participant_0008$p4079_i1_a0', 'participant_0008$p4079_i1_a1', 'participant_0008$p4079_i2_a0', 'participant_0008$p4079_i2_a1', 'participant_0008$p4079_i3_a0', 'participant_0008$p4079_i3_a1', 'participant_0008$p4080_i0_a0', 'participant_0008$p4080_i0_a1', 'participant_0008$p4080_i1_a0', 'participant_0008$p4080_i1_a1', 'participant_0008$p4080_i2_a0', 'participant_0008$p4080_i2_a1', 'participant_0008$p4080_i3_a0', 'participant_0008$p4080_i3_a1', 'participant_0042$p21001_i0', 'participant_0042$p21001_i1', 'participant_0042$p21001_i2', 'participant_0042$p21001_i3', 'participant_0042$p21002_i0', 'participant_0042$p21002_i1', 'participant_0042$p21002_i2', 'participant_0042$p21002_i3', 'participant_0049$p23099_i0', 'participant_0049$p23099_i1', 'participant_0049$p23099_i2', 'participant_0049$p23099_i3', 'participant_0050$p23127_i0', 'participant_0050$p23127_i1', 'participant_0050$p23127_i2', 'participant_0050$p23127_i3'] not found in ordered columns: ['participant$eid', 'participant$p31', 'participant$p21022', 'participant$p48_i0', 'participant$p48_i1', 'participant$p48_i2', 'participant$p48_i3', 'participant$p50_i0', 'participant$p50_i1', 'participant$p50_i2', 'participant$p50_i3', 'participant$p102_i0_a0', 'participant$p102_i0_a1', 'participant$p102_i1_a0', 'participant$p102_i1_a1', 'participant$p102_i2_a0', 'participant$p102_i2_a1', 'participant$p102_i3_a0', 'participant$p102_i3_a1', 'participant$p4079_i0_a0', 'participant$p4079_i0_a1', 'participant$p4079_i1_a0', 'participant$p4079_i1_a1', 'participant$p4079_i2_a0', 'participant$p4079_i2_a1', 'participant$p4079_i3_a0', 'participant$p4079_i3_a1', 'participant$p4080_i0_a0', 'participant$p4080_i0_a1', 'participant$p4080_i1_a0', 'participant$p4080_i1_a1', 'participant$p4080_i2_a0', 'participant$p4080_i2_a1', 'participant$p4080_i3_a0', 'participant$p4080_i3_a1', 'participant$p21001_i0', 'participant$p21001_i1', 'participant$p21001_i2', 'participant$p21001_i3', 'participant$p21002_i0', 'participant$p21002_i1', 'participant$p21002_i2', 'participant$p21002_i3', 'participant$p23099_i0', 'participant$p23099_i1', 'participant$p23099_i2', 'participant$p23099_i3', 'participant$p23127_i0', 'participant$p23127_i1', 'participant$p23127_i2', 'participant$p23127_i3']\n"
     ]
    }
   ],
   "source": [
    "metabolic = \"|\".join(\n",
    "    [\n",
    "        \"21002\",\n",
    "        \"50\",\n",
    "        \"48\",\n",
    "        \"21001\",\n",
    "        \"23099\",\n",
    "        \"23127\",\n",
    "        \"102\",\n",
    "        \"4080\",\n",
    "        \"4079\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "metabolic_fields = list(\n",
    "    participant.find_fields(lambda f: bool(re.match(f\"^p({metabolic})\\D\", f.name)))\n",
    ")\n",
    "\n",
    "field_names = (\n",
    "    [\"eid\"]\n",
    "    + [f.name for f in age_sex_fields]\n",
    "    + [f.name for f in metabolic_fields]\n",
    ")\n",
    "df = participant.retrieve_fields(names=field_names, engine=dxdata.connect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "14620ff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['xeid', 'x31_0_0', 'x21022_0_0', 'x48_0_0', 'x48_1_0', 'x48_2_0', 'x48_3_0', 'x50_0_0', 'x50_1_0', 'x50_2_0']\n"
     ]
    }
   ],
   "source": [
    "to_drop = [x for x in df.columns if \"a1\" in x]\n",
    "df = df.drop(*to_drop)\n",
    "colnames = [re.sub(\"_a\\d\", \"\", x) for x in df.columns]\n",
    "colnames = ['xeid'] + [new_names(s) for s in colnames[1:]]\n",
    "print(colnames[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e97258dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.toDF(*colnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "981feddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.csv(\"/tmp/phenos.tsv\", sep=\"\\t\", header=True, emptyValue='NA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "58c128f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['hadoop', 'fs', '-get', '/tmp/phenos.tsv', '../tmp/phenos.tsv'], returncode=0)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subprocess.run(\n",
    "    [\"hadoop\", \"fs\", \"-rm\", \"/tmp/phenos.tsv/_SUCCESS\"], check=True, shell=False\n",
    ")\n",
    "subprocess.run(\n",
    "    [\"hadoop\", \"fs\", \"-get\", \"/tmp/phenos.tsv\", \"../tmp/phenos.tsv\"],\n",
    "    check=True,\n",
    "    shell=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8a64d239",
   "metadata": {},
   "outputs": [],
   "source": [
    "!sed -e '3,${/^xeid/d' -e '}' ../tmp/phenos.tsv/part* > ../tmp/metabolic.QT.raw.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3181d1fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['dx', 'upload', '../tmp/metabolic.QT.raw.tsv', '--path', 'Data/phenotypes/'], returncode=0)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Upload to project\n",
    "\n",
    "subprocess.run(\n",
    "    [\"dx\", \"upload\", \"../tmp/metabolic.QT.raw.tsv\", \"--path\", \"Data/phenotypes/\"],\n",
    "    check=True,\n",
    "    shell=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c3c87b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
