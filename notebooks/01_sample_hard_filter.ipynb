{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f680db59",
   "metadata": {},
   "source": [
    "# Create hard sample filters\n",
    "\n",
    "For analysis, we need a cohort of samples with minimal population structure, minimal relatedness and without a few rare sources of error. This notebook generates a list of samples to remove from analysis in order to create such a cohort. We start out by importing stuff, initialising pyspark, setting various parameters from the configuration file, initialising Hail, and loading the participant dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cef860fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import subprocess\n",
    "\n",
    "import dxdata\n",
    "import dxpy\n",
    "import hail as hl\n",
    "import pyspark\n",
    "import tomli\n",
    "\n",
    "from utils import fields_for_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5735e492",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise Spark\n",
    "\n",
    "sc = pyspark.SparkContext()\n",
    "spark = pyspark.sql.SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48fa088d",
   "metadata": {
    "lines_to_next_cell": 2,
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Set configurations\n",
    "\n",
    "with open(\"../config.toml\", \"rb\") as f:\n",
    "    conf = tomli.load(f)\n",
    "\n",
    "RAW_REL_FILE = conf[\"SAMPLE_QC\"][\"UKB_REL_DAT_FILE\"]\n",
    "FINAL_FILTER_FILE = conf[\"SAMPLE_QC\"][\"SAMPLE_FILTER_FILE\"]\n",
    "\n",
    "MAX_KINSHIP = conf[\"SAMPLE_QC\"][\"MAX_KINSHIP\"]\n",
    "\n",
    "LOG_FILE = Path(conf[\"IMPORT\"][\"LOG_DIR\"], f\"sample_filters.log\").resolve().__str__()\n",
    "TMP_DIR = Path(conf[\"EXPORT\"][\"TMP_DIR\"])\n",
    "DATA_DIR = Path(conf[\"SAMPLE_QC\"][\"DATA_DIR\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4211a3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pip-installed Hail requires additional configuration options in Spark referring\n",
      "  to the path to the Hail Python module directory HAIL_DIR,\n",
      "  e.g. /path/to/python/site-packages/hail:\n",
      "    spark.jars=HAIL_DIR/hail-all-spark.jar\n",
      "    spark.driver.extraClassPath=HAIL_DIR/hail-all-spark.jar\n",
      "    spark.executor.extraClassPath=./hail-all-spark.jarRunning on Apache Spark version 2.4.4\n",
      "SparkUI available at http://ip-10-60-188-36.eu-west-2.compute.internal:8081\n",
      "Welcome to\n",
      "     __  __     <>__\n",
      "    / /_/ /__  __/ /\n",
      "   / __  / _ `/ / /\n",
      "  /_/ /_/\\_,_/_/_/   version 0.2.61-3c86d3ba497a\n",
      "LOGGING: writing to /opt/notebooks/gogoGPCR/hail_logs/sample_filters.log\n"
     ]
    }
   ],
   "source": [
    "# Initialise Hail\n",
    "\n",
    "hl.init(sc=sc, default_reference=\"GRCh38\", log=LOG_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "237cd766",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load participant dataset\n",
    "\n",
    "dispensed_database_name = dxpy.find_one_data_object(\n",
    "    classname=\"database\", name=\"app*\", folder=\"/\", name_mode=\"glob\", describe=True\n",
    ")[\"describe\"][\"name\"]\n",
    "dispensed_dataset_id = dxpy.find_one_data_object(\n",
    "    typename=\"Dataset\", name=\"app*.dataset\", folder=\"/\", name_mode=\"glob\"\n",
    ")[\"id\"]\n",
    "\n",
    "dataset = dxdata.load_dataset(id=dispensed_dataset_id)\n",
    "participant = dataset[\"participant\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c86ad27",
   "metadata": {},
   "source": [
    "# Filtering non-Caucasians and rare errors\n",
    "We filter out non-Caucasians (22006), outliers for heterozygosity or missing rate (22027), sex chromosome aneuploidy (22019) or genetic kinship to other participants (22021, UKB defined)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3115fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find relevant field names\n",
    "\n",
    "fields = [\"22027\", \"22019\", \"22006\", \"22021\"]\n",
    "field_names = [\n",
    "    fields_for_id(i, participant) for i in fields\n",
    "]  # fields_for_id(\"22027\") + fields_for_id(\"22019\") + fields_for_id(\"22006\") + fields_for_id(\"22021\")\n",
    "field_names = [\"eid\"] + [field.name for fields in field_names for field in fields]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52f3ef4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve dataframe\n",
    "\n",
    "df = participant.retrieve_fields(\n",
    "    names=field_names, engine=dxdata.connect(), coding_values=\"replace\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27cfe095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------+---------+--------------------------------+\n",
      "|eid    |p22027|p22019|p22006   |p22021                          |\n",
      "+-------+------+------+---------+--------------------------------+\n",
      "|3888244|null  |null  |Caucasian|No kinship found                |\n",
      "|1795659|null  |null  |Caucasian|No kinship found                |\n",
      "|2084720|null  |null  |Caucasian|At least one relative identified|\n",
      "|3742232|null  |null  |Caucasian|At least one relative identified|\n",
      "|1094442|null  |null  |Caucasian|At least one relative identified|\n",
      "+-------+------+------+---------+--------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a37cb7d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples to be filtered: 1815\n"
     ]
    }
   ],
   "source": [
    "# Use hard filters\n",
    "\n",
    "df = df.filter(\n",
    "    # df.p22006.isNull() | regenie should be able to handle population structure\n",
    "    (~df.p22027.isNull())\n",
    "    | (~df.p22019.isNull())\n",
    "    | (df.p22021 == \"Participant excluded from kinship inference process\")\n",
    "    | (df.p22021 == \"Ten or more third-degree relatives identified\")\n",
    ")\n",
    "filtered_samples_to_remove = hl.Table.from_spark(df.select(\"eid\")).key_by(\"eid\")\n",
    "print(f\"Samples to be filtered: {filtered_samples_to_remove.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036e7a23",
   "metadata": {},
   "source": [
    "# Filter related samples\n",
    "UK Biobank provides a list of genetically related individuals (KING) called 'ukb_rel.dat' which contains a kinship coefficient between pairs of individuals. Here, we remove any sample with a closer than 3rd degree relative (kinship > 0.088) and which is not already filtered out in the previous step. We then use Hail to create a maximal independent set of individuals by removing the smallest amount of related individuals. This is finally combined with the previously filtered samples to give the final list of samples to remove from the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2c6d57ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-30 09:17:17 Hail: INFO: Reading table to impute column types\n",
      "2021-11-30 09:17:20 Hail: INFO: Finished type imputation\n",
      "  Loading field 'ID1' as type str (user-supplied type)\n",
      "  Loading field 'ID2' as type str (user-supplied type)\n",
      "  Loading field 'HetHet' as type float64 (imputed)\n",
      "  Loading field 'IBS0' as type float64 (imputed)\n",
      "  Loading field 'Kinship' as type float64 (imputed)\n",
      "2021-11-30 09:17:20 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2021-11-30 09:17:20 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2021-11-30 09:17:23 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2021-11-30 09:17:23 Hail: INFO: Ordering unsorted dataset with network shuffle\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Related samples not already in filter and low kinship coefficient: 40096\n"
     ]
    }
   ],
   "source": [
    "# Import related table, remove any individual already sampled and keep those with kinship > 0.088\n",
    "\n",
    "rel = hl.import_table(\n",
    "    \"file:\" + \"/mnt/project/\" + RAW_REL_FILE,\n",
    "    delimiter=\" \",\n",
    "    impute=True,\n",
    "    types={\"ID1\": \"str\", \"ID2\": \"str\"},\n",
    ")\n",
    "\n",
    "rel = (\n",
    "    rel.key_by(\"ID2\")\n",
    "    .anti_join(filtered_samples_to_remove)\n",
    "    .key_by(\"ID1\")\n",
    "    .anti_join(filtered_samples_to_remove)\n",
    ")\n",
    "\n",
    "rel = rel.filter(rel.Kinship > MAX_KINSHIP, keep=True)\n",
    "\n",
    "print(\n",
    "    f\"Related samples not already in filter and low kinship coefficient: {rel.count()}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "05f28de9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-30 09:17:25 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2021-11-30 09:17:25 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2021-11-30 09:17:26 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2021-11-30 09:17:26 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2021-11-30 09:17:28 Hail: INFO: wrote table with 40096 rows in 1 partition to /tmp/T8jzHq8ke3YQas5IYM5l9s\n",
      "    Total size: 582.47 KiB\n",
      "    * Rows: 582.46 KiB\n",
      "    * Globals: 11.00 B\n",
      "    * Smallest partition: 40096 rows (582.46 KiB)\n",
      "    * Largest partition:  40096 rows (582.46 KiB)\n",
      "2021-11-30 09:17:30 Hail: INFO: Ordering unsorted dataset with network shuffle\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples to remove to create independent set: 34708\n"
     ]
    }
   ],
   "source": [
    "# Find maximal independent set\n",
    "\n",
    "related_samples_to_remove = (\n",
    "    hl.maximal_independent_set(\n",
    "        i=rel.ID1,\n",
    "        j=rel.ID2,\n",
    "        keep=False,\n",
    "    )\n",
    "    .rename({\"node\": \"eid\"})\n",
    "    .key_by(\"eid\")\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"Samples to remove to create independent set: {related_samples_to_remove.count()}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "57a2f94b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-30 09:17:30 Hail: INFO: Table.join: renamed the following fields on the right to avoid name conflicts:\n",
      "    'eid' -> 'eid_1'\n",
      "2021-11-30 09:17:30 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2021-11-30 09:17:30 Hail: INFO: Ordering unsorted dataset with network shuffle\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final number of samples to remove: 36523\n"
     ]
    }
   ],
   "source": [
    "# Join the two sets of samples to remove\n",
    "\n",
    "final = related_samples_to_remove.join(filtered_samples_to_remove, how=\"outer\")\n",
    "print(f\"Final number of samples to remove: {final.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e794f22c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-30 09:17:31 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2021-11-30 09:17:31 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2021-11-30 09:17:32 Hail: INFO: merging 17 files totalling 285.3K...\n",
      "2021-11-30 09:17:33 Hail: INFO: while writing:\n",
      "    file:/opt/notebooks/gogoGPCR/tmp/samples_to_remove.tsv\n",
      "  merge time: 134.934ms\n"
     ]
    }
   ],
   "source": [
    "# Export list\n",
    "\n",
    "FILTER_PATH = (TMP_DIR / FINAL_FILTER_FILE).resolve().__str__()\n",
    "PROCESSED_DIR = (DATA_DIR.parents[0].stem / Path(DATA_DIR.stem)).__str__() + \"/\"\n",
    "\n",
    "final.export(\"file:\" + FILTER_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b685e0fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['dx', 'upload', '/opt/notebooks/gogoGPCR/tmp/samples_to_remove.tsv', '--path', 'Data/filters/'], returncode=0)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Upload to project\n",
    "\n",
    "subprocess.run(\n",
    "    [\"dx\", \"upload\", FILTER_PATH, \"--path\", PROCESSED_DIR], check=True, shell=False\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
