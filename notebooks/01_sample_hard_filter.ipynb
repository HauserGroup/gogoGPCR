{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae4f605f",
   "metadata": {},
   "source": [
    "# Create hard sample filters\n",
    "\n",
    "For analysis, we need a cohort of samples with minimal population structure, minimal relatedness and without a few rare sources of error. This notebook generates a list of samples to remove from analysis in order to create such a cohort. We start out by importing stuff, initialising pyspark, setting various parameters from the configuration file, initialising Hail, and loading the participant dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e373bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from distutils.version import LooseVersion\n",
    "from pathlib import Path\n",
    "from subprocess import run\n",
    "\n",
    "import dxdata\n",
    "import dxpy\n",
    "import hail as hl\n",
    "import pyspark\n",
    "import toml\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "from src.utils import fields_for_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b85802d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise Spark\n",
    "\n",
    "sc = pyspark.SparkContext()\n",
    "spark = pyspark.sql.SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5146f415",
   "metadata": {
    "lines_to_next_cell": 2,
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Set configurations\n",
    "\n",
    "with open(\"../config.toml\") as f:\n",
    "    conf = toml.load(f)\n",
    "\n",
    "RAW_REL_FILE = conf[\"SAMPLE_QC\"][\"UKB_REL_DAT_FILE\"]\n",
    "FINAL_FILTER_FILE = conf[\"SAMPLE_QC\"][\"SAMPLE_FILTER_FILE\"]\n",
    "\n",
    "MAX_KINSHIP = conf[\"SAMPLE_QC\"][\"MAX_KINSHIP\"]\n",
    "\n",
    "LOG_FILE = Path(conf[\"IMPORT\"][\"LOG_DIR\"], f\"sample_filters.log\").resolve().__str__()\n",
    "TMP_DIR = Path(conf[\"EXPORT\"][\"TMP_DIR\"])\n",
    "DATA_DIR = Path(conf[\"SAMPLE_QC\"][\"DATA_DIR\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04088d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise Hail\n",
    "\n",
    "hl.init(sc=sc, default_reference=\"GRCh38\", log=LOG_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29233b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load participant dataset\n",
    "\n",
    "dispensed_database_name = dxpy.find_one_data_object(\n",
    "    classname=\"database\", name=\"app*\", folder=\"/\", name_mode=\"glob\", describe=True\n",
    ")[\"describe\"][\"name\"]\n",
    "dispensed_dataset_id = dxpy.find_one_data_object(\n",
    "    typename=\"Dataset\", name=\"app*.dataset\", folder=\"/\", name_mode=\"glob\"\n",
    ")[\"id\"]\n",
    "\n",
    "dataset = dxdata.load_dataset(id=dispensed_dataset_id)\n",
    "participant = dataset[\"participant\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c73394",
   "metadata": {},
   "source": [
    "# Filtering non-Caucasians and rare errors\n",
    "We filter out non-Caucasians (22006), outliers for heterozygosity or missing rate (22027), sex chromosome aneuploidy (22019) or genetic kinship to other participants (22021, UKB defined)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f57b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find relevant field names\n",
    "\n",
    "fields = [\"22027\", \"22019\", \"22006\", \"22021\"]\n",
    "field_names = [\n",
    "    fields_for_id(id) for id in fields\n",
    "]  # fields_for_id(\"22027\") + fields_for_id(\"22019\") + fields_for_id(\"22006\") + fields_for_id(\"22021\")\n",
    "field_names = [\"eid\"] + [field.name for fields in field_names for field in fields]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5d11f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve dataframe\n",
    "\n",
    "df = participant.retrieve_fields(\n",
    "    names=field_names, engine=dxdata.connect(), coding_values=\"replace\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550aaf3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4def37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use hard filters\n",
    "\n",
    "df = df.filter(\n",
    "    df.p22006.isNull()\n",
    "    | (~df.p22027.isNull())\n",
    "    | (~df.p22019.isNull())\n",
    "    | (df.p22021 == \"Participant excluded from kinship inference process\")\n",
    "    | (df.p22021 == \"Ten or more third-degree relatives identified\")\n",
    ")\n",
    "filtered_samples_to_remove = hl.Table.from_spark(df.select(\"eid\")).key_by(\"eid\")\n",
    "print(f\"Samples to be filtered: {filtered_samples_to_remove.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c482be9",
   "metadata": {},
   "source": [
    "# Filter related samples\n",
    "UK Biobank provides a list of genetically related individuals (KING) called 'ukb_rel.dat' which contains a kinship coefficient between pairs of individuals. Here, we remove any sample with a closer than 3rd degree relative (kinship > 0.088) and which is not already filtered out in the previous step. We then use Hail to create a maximal independent set of individuals by removing the smallest amount of related individuals. This is finally combined with the previously filtered samples to give the final list of samples to remove from the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44c6f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import related table, remove any individual already sampled and keep those with kinship > 0.088\n",
    "\n",
    "rel = hl.import_table(\n",
    "    \"file:\" + \"/mnt/project/\" + RAW_REL_FILE,\n",
    "    delimiter=\" \",\n",
    "    impute=True,\n",
    "    types={\"ID1\": \"str\", \"ID2\": \"str\"},\n",
    ")\n",
    "\n",
    "rel = (\n",
    "    rel.key_by(\"ID2\")\n",
    "    .anti_join(filtered_samples_to_remove)\n",
    "    .key_by(\"ID1\")\n",
    "    .anti_join(filtered_samples_to_remove)\n",
    ")\n",
    "\n",
    "rel = rel.filter(rel.Kinship > MAX_KINSHIP, keep=True)\n",
    "\n",
    "print(\n",
    "    f\"Related samples not already in filter and low kinship coefficient: {rel.count()}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6017af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find maximal independent set\n",
    "\n",
    "related_samples_to_remove = (\n",
    "    hl.maximal_independent_set(\n",
    "        i=rel.ID1,\n",
    "        j=rel.ID2,\n",
    "        keep=False,\n",
    "    )\n",
    "    .rename({\"node\": \"eid\"})\n",
    "    .key_by(\"eid\")\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"Samples to remove to create independent set: {related_samples_to_remove.count()}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d81a0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the two sets of samples to remove\n",
    "\n",
    "final = related_samples_to_remove.join(filtered_samples_to_remove, how=\"outer\")\n",
    "print(f\"Final number of samples to remove: {final.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a2d3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export list\n",
    "\n",
    "FILTER_PATH = (TMP_DIR / FINAL_FILTER_FILE).resolve().__str__()\n",
    "PROCESSED_DIR = (DATA_DIR.parents[0].stem / Path(DATA_DIR.stem)).__str__() + \"/\"\n",
    "\n",
    "final.export(\"file:\" + FILTER_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a035dadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload to project\n",
    "\n",
    "run([\"dx\", \"upload\", FILTER_PATH, \"--path\", PROCESSED_DIR], check=True, shell=False)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
