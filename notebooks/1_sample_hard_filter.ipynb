{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5eea9146",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: toml in /opt/conda/lib/python3.6/site-packages (0.10.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install toml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbfbc9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import dxpy\n",
    "import dxdata\n",
    "import toml\n",
    "\n",
    "from distutils.version import LooseVersion\n",
    "from subprocess import run\n",
    "from pathlib import Path\n",
    "\n",
    "import hail as hl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0614dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = pyspark.SparkContext()\n",
    "spark = pyspark.sql.SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "94656080",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "with open(\"../config.toml\") as f:\n",
    "    conf = toml.load(f)\n",
    "\n",
    "RAW_REL_FILE = conf[\"SAMPLE_QC\"][\"UKB_REL_DAT_FILE\"]\n",
    "FINAL_FILTER_FILE = conf[\"SAMPLE_QC\"][\"SAMPLE_FILTER_FILE\"]\n",
    "\n",
    "MAX_KINSHIP = conf[\"SAMPLE_QC\"][\"MAX_KINSHIP\"]\n",
    "\n",
    "LOG_FILE = Path(conf[\"IMPORT\"][\"LOG_DIR\"], f\"sample_filters.log\").resolve().__str__()\n",
    "TMP_DIR = Path(conf[\"EXPORT\"][\"TMP_DIR\"])\n",
    "DATA_DIR = Path(conf[\"SAMPLE_QC\"][\"DATA_DIR\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12ca5e12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pip-installed Hail requires additional configuration options in Spark referring\n",
      "  to the path to the Hail Python module directory HAIL_DIR,\n",
      "  e.g. /path/to/python/site-packages/hail:\n",
      "    spark.jars=HAIL_DIR/hail-all-spark.jar\n",
      "    spark.driver.extraClassPath=HAIL_DIR/hail-all-spark.jar\n",
      "    spark.executor.extraClassPath=./hail-all-spark.jarRunning on Apache Spark version 2.4.4\n",
      "SparkUI available at http://ip-10-60-0-47.eu-west-2.compute.internal:8081\n",
      "Welcome to\n",
      "     __  __     <>__\n",
      "    / /_/ /__  __/ /\n",
      "   / __  / _ `/ / /\n",
      "  /_/ /_/\\_,_/_/_/   version 0.2.61-3c86d3ba497a\n",
      "LOGGING: writing to /opt/notebooks/gogoGPCR/hail_logs/sample_filters.log\n"
     ]
    }
   ],
   "source": [
    "hl.init(sc=sc, default_reference='GRCh38', log=LOG_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3ce1b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "dispensed_database_name = dxpy.find_one_data_object(classname=\"database\", name=\"app*\", folder=\"/\", name_mode=\"glob\", describe=True)[\"describe\"][\"name\"]\n",
    "dispensed_dataset_id = dxpy.find_one_data_object(typename=\"Dataset\", name=\"app*.dataset\", folder=\"/\", name_mode=\"glob\")[\"id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9360280e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dxdata.load_dataset(id=dispensed_dataset_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a272a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "participant = dataset[\"participant\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "313204fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fields_for_id(field_id):\n",
    "\n",
    "    field_id = str(field_id)\n",
    "    fields = participant.find_fields(name_regex=r'^p{}(_i\\d+)?(_a\\d+)?$'.format(field_id))\n",
    "    \n",
    "    return sorted(fields, key=lambda f: LooseVersion(f.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a69b289b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = [\"22027\", \"22019\", \"22006\", \"22021\"]\n",
    "field_names = [fields_for_id(id) for id in fields] #fields_for_id(\"22027\") + fields_for_id(\"22019\") + fields_for_id(\"22006\") + fields_for_id(\"22021\")\n",
    "field_names = [\"eid\"] + [field.name for fields in field_names for field in fields]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "055683ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = participant.retrieve_fields(names=field_names, engine=dxdata.connect(), coding_values=\"replace\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "1ca2b767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------+---------+--------------------------------+\n",
      "|eid    |p22027|p22019|p22006   |p22021                          |\n",
      "+-------+------+------+---------+--------------------------------+\n",
      "|3888244|null  |null  |Caucasian|No kinship found                |\n",
      "|1795659|null  |null  |Caucasian|No kinship found                |\n",
      "|2084720|null  |null  |Caucasian|At least one relative identified|\n",
      "|3742232|null  |null  |Caucasian|At least one relative identified|\n",
      "|1094442|null  |null  |Caucasian|At least one relative identified|\n",
      "+-------+------+------+---------+--------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "0fb59926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples to be filtered: 94317\n"
     ]
    }
   ],
   "source": [
    "df = df.filter(df.p22006.isNull() |\n",
    "          (~df.p22027.isNull()) | \n",
    "          (~df.p22019.isNull()) |\n",
    "          (df.p22021 == \"Participant excluded from kinship inference process\") | \n",
    "          (df.p22021 == \"Ten or more third-degree relatives identified\")\n",
    "              )\n",
    "filtered_samples_to_remove = hl.Table.from_spark(df.select(\"eid\")).key_by(\"eid\")\n",
    "print(f\"Samples to be filtered: {filtered_samples_to_remove.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "08396ff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-08 10:45:23 Hail: INFO: Reading table to impute column types\n",
      "2021-11-08 10:45:24 Hail: INFO: Finished type imputation\n",
      "  Loading field 'ID1' as type str (user-supplied type)\n",
      "  Loading field 'ID2' as type str (user-supplied type)\n",
      "  Loading field 'HetHet' as type float64 (imputed)\n",
      "  Loading field 'IBS0' as type float64 (imputed)\n",
      "  Loading field 'Kinship' as type float64 (imputed)\n",
      "2021-11-08 10:45:24 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2021-11-08 10:45:25 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2021-11-08 10:45:26 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2021-11-08 10:45:26 Hail: INFO: Ordering unsorted dataset with network shuffle\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Related samples not already in filter and low kinship coefficient: 34111\n"
     ]
    }
   ],
   "source": [
    "rel = hl.import_table(\n",
    "        \"file:\" + \"/mnt/project/\" + RAW_REL_FILE,\n",
    "        delimiter=\" \",\n",
    "        impute=True,\n",
    "        types={\"ID1\": \"str\", \"ID2\": \"str\"},\n",
    "    )\n",
    "\n",
    "rel = rel.key_by(\"ID2\").anti_join(filtered_samples_to_remove).key_by(\"ID1\").anti_join(filtered_samples_to_remove)\n",
    "\n",
    "rel = rel.filter(rel.Kinship > MAX_KINSHIP, keep=True)\n",
    "\n",
    "print(f\"Related samples not already in filter and low kinship coefficient: {rel.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "a8cc8f09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-08 10:46:10 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2021-11-08 10:46:10 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2021-11-08 10:46:12 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2021-11-08 10:46:12 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2021-11-08 10:46:14 Hail: INFO: wrote table with 34111 rows in 1 partition to /tmp/BR3e9VXPApCLzmLnkXrL7m\n",
      "    Total size: 497.10 KiB\n",
      "    * Rows: 497.09 KiB\n",
      "    * Globals: 11.00 B\n",
      "    * Smallest partition: 34111 rows (497.09 KiB)\n",
      "    * Largest partition:  34111 rows (497.09 KiB)\n",
      "2021-11-08 10:46:15 Hail: INFO: Ordering unsorted dataset with network shuffle\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples to remove to create independent set: 29667\n"
     ]
    }
   ],
   "source": [
    "#Find maximal independent set\n",
    "\n",
    "related_samples_to_remove = hl.maximal_independent_set(\n",
    "    i=rel.ID1,\n",
    "    j=rel.ID2,\n",
    "    keep=False,\n",
    ").rename({\"node\": \"eid\"}).key_by(\"eid\")\n",
    "\n",
    "print(f\"Samples to remove to create independent set: {related_samples_to_remove.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "3a853926",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-08 10:58:58 Hail: INFO: Table.join: renamed the following fields on the right to avoid name conflicts:\n",
      "    'eid' -> 'eid_1'\n",
      "2021-11-08 10:58:58 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2021-11-08 10:58:58 Hail: INFO: Ordering unsorted dataset with network shuffle\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final number of samples to remove: 123984\n"
     ]
    }
   ],
   "source": [
    "final = related_samples_to_remove.join(filtered_samples_to_remove, how=\"outer\" )\n",
    "print(f\"Final number of samples to remove: {final.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "f739249e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-08 11:14:41 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2021-11-08 11:14:41 Hail: INFO: merging 1 files totalling 231.8K...\n",
      "2021-11-08 11:14:41 Hail: INFO: while writing:\n",
      "    file:/opt/notebooks/gogoGPCR/tmp/samples_to_remove.tsv\n",
      "  merge time: 10.514ms\n"
     ]
    }
   ],
   "source": [
    "FILTER_PATH = (TMP_DIR / FINAL_FILTER_FILE).resolve().__str__()\n",
    "PROCESSED_DIR = (DATA_DIR.parents[0].stem / Path(DATA_DIR.stem)).__str__()  + \"/\"\n",
    "\n",
    "related_samples_to_remove.export(\"file:\" + FILTER_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "4fbea021",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['dx', 'upload', '/opt/notebooks/gogoGPCR/tmp/samples_to_remove.tsv', '--path', 'data/processed/'], returncode=0)"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run([\"dx\", \"upload\", FILTER_PATH, \"--path\", PROCESSED_DIR], check = True, shell = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
